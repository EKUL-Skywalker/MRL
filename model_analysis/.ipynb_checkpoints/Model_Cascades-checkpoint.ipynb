{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154e9d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e6746",
   "metadata": {},
   "source": [
    "# Threshold based model cascades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a849e341",
   "metadata": {},
   "source": [
    "#### Greedy heuristic to jump between consecutive rep. sizes in MRL models is determined using model's prediction confidence. For each rep. size we first determine threshold on prediction confidence, such that, if for any input image if the confidence is below threshold, we jump to the next model. We perform a grid search on interval (0, 1) using a held out set of size 10000, and report result on remaining 40000 images. Lastly, in the paper we report results averaged over 30 random seeds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72301411",
   "metadata": {},
   "source": [
    " #### Note that to have a reliable estimation of thresholds, we use test time augmentation, therefore, inference scripts MUST be run using `--tta` flag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c55295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_seeds=30\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_GROUND_TRUTH = \"gt_dataset=V1.pth\" # we need to pass ground truth as a torch tensor. \n",
    "PATH_TO_SOFTMAX_PROBABILITIES = \"mrl=1_efficient=0_dataset=V1_tta=1_softmax.pth\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526238c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_idxes = [];thrsh_all_seeds=[]\n",
    "for l in range(N_seeds):\n",
    "    idx = np.random.choice(50000, 10000, replace=False) # Selecting the subset for grid search.\n",
    "    input_idxes.append(idx) # appending it so that we can get the held out set. \n",
    "    greedy_thrsh=[]\n",
    "    gt = torch.load(PATH_TO_GROUND_TRUTH)[idx].cpu()\n",
    "    softmax=torch.load(PATH_TO_SOFTMAX_PROBABILITIES)[:, idx, :].cpu()\n",
    "    confidence_, predictions_ = torch.max(softmax, dim=-1)\n",
    "    n=len(gt)\n",
    "\n",
    "    thrsh = np.linspace(0.1,1, 100) # Grid search\n",
    "    for d1 in range(8):\n",
    "        d2=d1+1\n",
    "        print(\"Searching thresholds between models of rep. size\", 2**(d1+3), 2**(d2+3))\n",
    "        confidence_d1, predictions_d1 = confidence_[d1], predictions_[d1]\n",
    "        confidence_d2, predictions_d2 = confidence_[d2], predictions_[d2]\n",
    "\n",
    "        acc=[]; lower_=[]\n",
    "        for t in thrsh:\n",
    "            preds= torch.zeros(n); preds=preds.long()\n",
    "            idx_d1 = (confidence_d1>t) # indices where smaller dimension is confident than threshold\n",
    "            idx_d2 =(confidence_d1<=t) \n",
    "            n1, n2= (idx_d1.sum()).item(), (idx_d2.sum()).item() # number of such examples.\n",
    "            preds[idx_d1] = predictions_d1[idx_d1] # Using predictions from smaller dimension\n",
    "            preds[idx_d2] = predictions_d2[idx_d2] # Using predictions from higher dimension\n",
    "            acc.append(100*(((preds==gt).sum())/n).cpu().numpy()) # Computing accuracy \n",
    "\n",
    "        acc = np.asarray(acc)\n",
    "        max_acc = -np.asarray(sorted(-acc))[0]\n",
    "        best_thrhs = thrsh[np.nonzero(acc==max_acc)[0]].min() # Choosing minimum such threshold which will result in best accuracy\n",
    "        max_idx =(np.asarray(acc)).argmax()\n",
    "        print(f\"Cascade Performance between dimension {2**(d1+3)} and {2**(d2+3)} is {acc[max_idx]} with threshold {best_thrhs}\")\n",
    "        greedy_thrsh.append(best_thrhs) # Saving the policy  \n",
    "    \n",
    "    thrsh_all_seeds.append(greedy_thrsh) # For each random seed.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af19d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = [] # Getting the held out set for testing greedy policy.\n",
    "for i in input_idxes:\n",
    "    u = np.zeros(50000)\n",
    "    u[i]=1\n",
    "    sel.append(np.nonzero(1-u)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56c56a",
   "metadata": {},
   "source": [
    "### Evaluating Greedy Policy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da53d1ee",
   "metadata": {},
   "source": [
    "#### In the previous snippet we determined the greedy threshold to switch between the consecutive rep. sizes. We can now use this to naviagate between the cascades. While we can do this for all the models (that is rep. sizes from 8 to 2048), but we will also study how does early stopping affects the performance. This means, we will set a cap on the maximum rep. size we will use in this cascade system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c659d85c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_expected_dimensions=[[] for l in range(8)] # For every max rep. size for cascading, we store the expected dimension used for prediction to later average across random seeds.\n",
    "all_accuracies=[[] for l in range(8)]\n",
    "\n",
    "for early_stopping_dim in range(8): # Early stopping dimension\n",
    "    print(early_stopping_dim+1)\n",
    "    for seed in range(N_seeds):\n",
    "        gt = torch.load(PATH_TO_GROUND_TRUTH)[sel[seed]].cpu() #choosing the held out set\n",
    "        softmax=torch.load(PATH_TO_SOFTMAX_PROBABILITIES)[:, sel[seed], :].cpu() # corresponding softmaxes\n",
    "        confidence_, predictions_ = torch.max(softmax, dim=-1)\n",
    "        greedy_thrsh = thrsh_all_seeds[seed] # Greedy policy for that seed\n",
    "\n",
    "        max_cascading = early_stopping_dim+1\n",
    "        acc=0 #number of correct predictions\n",
    "        model=[] \n",
    "        for i in tqdm(range(predictions_.shape[-1])): #iterating over all the testing examples. \n",
    "            flag=True\n",
    "            for j in range(max_cascading):\n",
    "                if confidence_[j, i] > greedy_thrsh[j]: # if we are confident at smaller dimension model, then break\n",
    "                    j_ = j; flag=False # j_ denotes the dim we used to make prediction. \n",
    "                    break\n",
    "            if flag:\n",
    "                j_=max_cascading # This means that we will use the maximum possible dimension for predictions. \n",
    "                \n",
    "            model.append(j_) # dimension to be used for prediction\n",
    "            acc+=(predictions_[j_, i]==gt[i]).sum()\n",
    "            \n",
    "        counter=collections.Counter(model) # A counter over the models used for predicting    \n",
    "        probs = {2**(j+3): counter[j]/len(gt) for j in counter.keys()} # probability distribution, used to compute expected representation size for prediction \n",
    "\n",
    "        expected_dim=0\n",
    "        for k in probs.keys():\n",
    "            expected_dim+= k*probs[k]\n",
    "\n",
    "        all_expected_dimensions[early_stopping_dim].append([expected_dim]) # Saving expected dimensionality for every seed and maximum cascade\n",
    "        all_accuracies[early_stopping_dim].append([(acc/len(gt)).item()])  # Saving accuracy for every seed and maximum cascade\n",
    "\n",
    "all_expected_dimensions = (np.asarray(all_expected_dimensions)).squeeze()\n",
    "all_accuracies = (np.asarray(all_accuracies)).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19efe3a7",
   "metadata": {},
   "source": [
    "### Expected dimension statistics for different maximum cascade rep. size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9192314a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 13.18502     18.02794     25.73146     35.28671333  45.66122\n",
      "  62.05503333  87.92042    121.67231333]\n",
      "[ 0.72321347  1.60812216  2.80969956  5.28050878  7.60811513 10.94328704\n",
      " 20.18233146 32.94076334]\n"
     ]
    }
   ],
   "source": [
    "print(all_expected_dimensions.mean(axis=-1)) # Mean expected dimension for every maximum cascade rep. size\n",
    "print(all_expected_dimensions.std(axis=-1)) # standard deviation in expected dimension for every maximum cascade rep. size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd57e60e",
   "metadata": {},
   "source": [
    "### Cascade accuracy statistics for different maximum cascade rep. size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c6110c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73790917 0.75242666 0.76048249 0.76273834 0.76418666 0.76514667\n",
      " 0.76533666 0.76541667]\n",
      "[0.00103216 0.00091551 0.00139763 0.00170652 0.00188398 0.0019996\n",
      " 0.002036   0.0020418 ]\n"
     ]
    }
   ],
   "source": [
    "print(all_accuracies.mean(axis=-1))\n",
    "print(all_accuracies.std(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86bc715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "thrsh_all_seeds = np.asarray(thrsh_all_seeds) # threshold for different random seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfee5f0",
   "metadata": {},
   "source": [
    "### Greedy threshold statistics for different maximum cascade rep. size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b1f5aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78636364 0.70939394 0.7930303  0.65151515 0.55333333 0.55636364\n",
      " 0.52212121 0.44484848]\n",
      "[0.07305796 0.11164888 0.07465931 0.1491451  0.05973926 0.06952812\n",
      " 0.13982837 0.11729113]\n"
     ]
    }
   ],
   "source": [
    "print(thrsh_all_seeds.mean(axis=0))\n",
    "print(thrsh_all_seeds.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8f04d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
